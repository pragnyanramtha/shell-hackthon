{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12405754,"sourceType":"datasetVersion","datasetId":7823452},{"sourceId":12406847,"sourceType":"datasetVersion","datasetId":7824237},{"sourceId":12513943,"sourceType":"datasetVersion","datasetId":7898717}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0a4bf7ee","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.multioutput import MultiOutputRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nfrom scipy.stats import pearsonr\nimport warnings\nimport time\n\n# Attempt to import cupy for GPU acceleration of data arrays\ntry:\n    import cupy as cp\n    CUPY_AVAILABLE = True\n    print(\"CuPy found. GPU data acceleration is enabled.\")\nexcept ImportError:\n    CUPY_AVAILABLE = False\n    print(\"CuPy not found. Predictions will use CPU data, which may be slower.\")\n\nwarnings.filterwarnings('ignore')\n\n# Set device for GPU acceleration for PyTorch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using PyTorch device: {device}\")\n\n# =========================================================================\n# Helper Classes\n# =========================================================================\n\nclass FuelBlendingDataset(Dataset):\n    def __init__(self, X, y=None):\n        self.X = torch.FloatTensor(X.values if isinstance(X, pd.DataFrame) else X)\n        self.y = torch.FloatTensor(y.values if isinstance(y, pd.DataFrame) else y) if y is not None else None\n    def __len__(self): return len(self.X)\n    def __getitem__(self, idx):\n        return (self.X[idx], self.y[idx]) if self.y is not None else self.X[idx]\n\nclass MultiTargetModelWrapper:\n    \"\"\"A generic wrapper for multi-target models trained individually.\"\"\"\n    def __init__(self, models):\n        self.models = models\n\n    def predict(self, X):\n        predictions = [model.predict(X) for model in self.models]\n        if isinstance(predictions[0], np.ndarray):\n            return np.column_stack(predictions)\n        elif CUPY_AVAILABLE and isinstance(predictions[0], cp.ndarray):\n            return cp.column_stack(predictions)\n        return np.column_stack(predictions)\n\nclass AttentionLayer(nn.Module):\n    def __init__(self, input_dim, attention_dim=64):\n        super(AttentionLayer, self).__init__()\n        self.attention = nn.Sequential(nn.Linear(input_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim, 1, bias=False))\n    def forward(self, x):\n        weights = torch.softmax(self.attention(x), dim=1)\n        return x * weights\n\nclass AdvancedNeuralNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dims=[512, 256, 128, 64], dropout_rate=0.3):\n        super(AdvancedNeuralNetwork, self).__init__()\n        layers = [nn.Linear(input_dim, hidden_dims[0]), nn.BatchNorm1d(hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout_rate)]\n        for i in range(len(hidden_dims) - 1):\n            layers.extend([nn.Linear(hidden_dims[i], hidden_dims[i+1]), nn.BatchNorm1d(hidden_dims[i+1]), nn.ReLU(), nn.Dropout(dropout_rate)])\n        self.main_layers = nn.Sequential(*layers)\n        self.attention = AttentionLayer(hidden_dims[-1])\n        self.output_layers = nn.Sequential(nn.Linear(hidden_dims[-1], hidden_dims[-1]//2), nn.ReLU(), nn.Dropout(dropout_rate/2), nn.Linear(hidden_dims[-1]//2, output_dim))\n        self.residual = nn.Linear(input_dim, output_dim)\n    def forward(self, x):\n        main_out = self.attention(self.main_layers(x))\n        main_pred = self.output_layers(main_out)\n        resid_pred = self.residual(x)\n        alpha = torch.sigmoid(main_pred.mean(dim=1, keepdim=True))\n        return alpha * main_pred + (1 - alpha) * resid_pred\n\n# =========================================================================\n# Main Predictor Class\n# =========================================================================\n\nclass FuelBlendingPredictor:\n    def __init__(self, use_gpu=True):\n        self.device = device if use_gpu and torch.cuda.is_available() else torch.device('cpu')\n        self.models = {}\n        self.scalers = {}\n        self.neural_net = None\n        self.best_features = None\n        self.target_names = None\n        \n    def create_advanced_features(self, df):\n        df_features = df.copy()\n        blend_cols = [col for col in df.columns if 'fraction' in col]\n        if not blend_cols: return df_features # Return if no fraction columns found\n        df_features['blend_entropy'] = -np.sum(df[blend_cols] * np.log(df[blend_cols] + 1e-8), axis=1)\n        df_features['blend_gini'] = 1 - np.sum(df[blend_cols]**2, axis=1)\n        return df_features\n    \n    def select_best_features(self, X, y, max_features=150):\n        X = X.fillna(0) # Simple imputation for feature selection\n        feature_scores = pd.Series(index=X.columns, dtype=float).fillna(0)\n        mi_scores = mutual_info_regression(X, y.mean(axis=1), random_state=42)\n        feature_scores += pd.Series(mi_scores, index=X.columns)\n        f_selector = SelectKBest(score_func=f_regression, k='all')\n        f_selector.fit(X, y.mean(axis=1))\n        feature_scores += pd.Series(f_selector.scores_ / 1000, index=X.columns).fillna(0)\n        selected_features = feature_scores.nlargest(max_features).index.tolist()\n        print(f\"Selected {len(selected_features)} features out of {len(X.columns)}\")\n        return selected_features\n    \n    def train_neural_network(self, X_train, y_train, X_val, y_val, epochs=250, batch_size=64, lr=0.001):\n        print(\"Training Neural Network...\")\n        train_loader = DataLoader(FuelBlendingDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(FuelBlendingDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n        self.neural_net = AdvancedNeuralNetwork(X_train.shape[1], y_train.shape[1]).to(self.device)\n        criterion = nn.MSELoss()\n        optimizer = optim.AdamW(self.neural_net.parameters(), lr=lr, weight_decay=1e-4)\n        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=15, factor=0.5)\n        best_val_loss = float('inf')\n        patience_counter, patience = 0, 30\n        \n        for epoch in range(epochs):\n            self.neural_net.train()\n            for X_batch, y_batch in train_loader:\n                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n                optimizer.zero_grad()\n                loss = criterion(self.neural_net(X_batch), y_batch)\n                loss.backward()\n                optimizer.step()\n            \n            self.neural_net.eval()\n            val_loss = sum(criterion(self.neural_net(X.to(self.device)), y.to(self.device)).item() for X, y in val_loader) / len(val_loader)\n            scheduler.step(val_loss)\n            \n            if val_loss < best_val_loss:\n                best_val_loss, patience_counter = val_loss, 0\n                torch.save(self.neural_net.state_dict(), 'best_neural_net.pth')\n            else:\n                patience_counter += 1\n                if patience_counter >= patience:\n                    print(f\"Early stopping at epoch {epoch}\")\n                    break\n        \n        self.neural_net.load_state_dict(torch.load('best_neural_net.pth'))\n        print(f\"Neural Network training completed. Best validation loss: {best_val_loss:.6f}\")\n    \n    def train_gradient_boosting_models(self, X_train, y_train, X_val, y_val):\n        print(\"Training Gradient Boosting Models...\")\n        X_val_device = cp.asarray(X_val) if (self.device.type == 'cuda' and CUPY_AVAILABLE) else X_val\n\n        # --- Per-target training for XGBoost ---\n        print(\"Training xgb with per-target early stopping...\")\n        start_time = time.time()\n        xgb_params = {'n_estimators': 2000, 'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.8,\n                      'colsample_bytree': 0.8, 'random_state': 42, 'tree_method': 'hist',\n                      'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n                      'early_stopping_rounds': 50} # Correct way to set early stopping\n        xgb_models = []\n        for i in range(y_train.shape[1]):\n            model = xgb.XGBRegressor(**xgb_params)\n            model.fit(X_train, y_train.iloc[:, i],\n                      eval_set=[(X_val, y_val.iloc[:, i])], verbose=False)\n            xgb_models.append(model)\n        self.models['xgb'] = MultiTargetModelWrapper(xgb_models)\n        print(f\"xgb training completed in {time.time() - start_time:.2f}s.\")\n\n        # --- Per-target training for CatBoost ---\n        print(\"Training catboost with per-target early stopping...\")\n        start_time = time.time()\n        cb_params = {'iterations': 2000, 'learning_rate': 0.05, 'depth': 7, 'l2_leaf_reg': 3,\n                     'random_seed': 42, 'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n                     'verbose': 0, 'allow_writing_files': False, 'early_stopping_rounds': 50}\n        cb_models = []\n        for i in range(y_train.shape[1]):\n            model = cb.CatBoostRegressor(**cb_params)\n            model.fit(X_train, y_train.iloc[:, i],\n                      eval_set=[(X_val, y_val.iloc[:, i])], verbose=False)\n            cb_models.append(model)\n        self.models['catboost'] = MultiTargetModelWrapper(cb_models)\n        print(f\"catboost training completed in {time.time() - start_time:.2f}s.\")\n\n        # --- Per-target training for LightGBM ---\n        print(\"Training lgb with per-target early stopping...\")\n        start_time = time.time()\n        lgb_params = {'n_estimators': 2500, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 8,\n                      'max_bin': 128, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42,\n                      'device': 'gpu' if torch.cuda.is_available() else 'cpu', 'verbose': -1}\n        lgb_models = []\n        for i in range(y_train.shape[1]):\n            model = lgb.LGBMRegressor(**lgb_params)\n            model.fit(X_train, y_train.iloc[:, i],\n                      eval_set=[(X_val, y_val.iloc[:, i])],\n                      callbacks=[lgb.early_stopping(50, verbose=False)])\n            lgb_models.append(model)\n        self.models['lgb'] = MultiTargetModelWrapper(lgb_models)\n        print(f\"lgb training completed in {time.time() - start_time:.2f}s.\")\n\n        # --- Training for RandomForest ---\n        print(\"Training rf...\")\n        start_time = time.time()\n        rf = RandomForestRegressor(n_estimators=150, max_depth=12, min_samples_leaf=3, random_state=42, n_jobs=-1)\n        self.models['rf'] = MultiOutputRegressor(rf, n_jobs=-1).fit(X_train, y_train)\n        print(f\"rf training completed in {time.time() - start_time:.2f}s.\")\n    \n    def create_ensemble_predictions(self, X):\n        predictions, weights = [], []\n        use_cupy = self.device.type == 'cuda' and CUPY_AVAILABLE\n        X_device = cp.asarray(X) if use_cupy else X\n\n        if self.neural_net:\n            self.neural_net.eval()\n            with torch.no_grad():\n                nn_pred = self.neural_net(torch.FloatTensor(X.values).to(self.device)).cpu().numpy()\n                predictions.append(nn_pred)\n                weights.append(0.35)\n        \n        for name, model in self.models.items():\n            is_gpu_model = name in ['xgb', 'lgb', 'catboost']\n            pred_device = model.predict(X_device if (use_cupy and is_gpu_model) else X)\n            pred = pred_device.get() if (use_cupy and isinstance(pred_device, cp.ndarray)) else pred_device\n            predictions.append(pred)\n            weights.append(0.20 if is_gpu_model else 0.05)\n        \n        return pd.DataFrame(np.average(predictions, axis=0, weights=np.array(weights)/np.sum(weights)), columns=self.target_names)\n    \n    def fit(self, X, y):\n        self.target_names = y.columns.tolist()\n        X_processed = self.create_advanced_features(X)\n        self.best_features = self.select_best_features(X_processed, y)\n        X_selected = X_processed[self.best_features]\n        \n        self.scalers['feature_scaler'] = RobustScaler()\n        X_scaled = pd.DataFrame(self.scalers['feature_scaler'].fit_transform(X_selected), columns=self.best_features)\n        self.scalers['target_scaler'] = RobustScaler()\n        y_scaled = pd.DataFrame(self.scalers['target_scaler'].fit_transform(y), columns=y.columns)\n        \n        X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n        \n        self.train_neural_network(X_train, y_train, X_val, y_val)\n        self.train_gradient_boosting_models(X_train, y_train, X_val, y_val)\n        \n        val_preds_scaled = self.create_ensemble_predictions(X_val)\n        val_preds_orig = self.scalers['target_scaler'].inverse_transform(val_preds_scaled)\n        y_val_orig = self.scalers['target_scaler'].inverse_transform(y_val)\n        final_mape = mean_absolute_percentage_error(y_val_orig, val_preds_orig)\n        print(f\"\\n🎯 Final Ensemble MAPE on Validation Set: {final_mape:.6f}\")\n        return self\n    \n    def predict(self, X):\n        X_processed = self.create_advanced_features(X)\n        X_selected = X_processed[self.best_features]\n        X_scaled = pd.DataFrame(self.scalers['feature_scaler'].transform(X_selected), columns=self.best_features)\n        preds_scaled = self.create_ensemble_predictions(X_scaled)\n        return pd.DataFrame(self.scalers['target_scaler'].inverse_transform(preds_scaled), columns=self.target_names)\n\n# =========================================================================\n# Main Execution Block\n# =========================================================================\n\ndef main():\n    print(\"🚀 Starting Fuel Blending ML Pipeline\")\n    try:\n        # Updated data loading paths\n        train_df = pd.read_csv('/kaggle/input/training/train.csv')\n        test_df = pd.read_csv('/kaggle/input/testing/test.csv')\n        sample_submission = pd.read_csv('/kaggle/input/samplesubmission/sample_solution.csv')\n        \n    except FileNotFoundError as e:\n        print(f\"Error loading data: {e}\")\n        print(\"Please ensure the Kaggle dataset is correctly mounted at /kaggle/input/\")\n        return\n        \n    target_columns = [col for col in train_df.columns if 'BlendProperty' in col]\n    if not target_columns:\n        # Fallback for different naming conventions\n        target_columns = sample_submission.columns.drop('ID').tolist()\n\n    feature_columns = [col for col in train_df.columns if col not in target_columns and 'ID' not in col]\n    \n    # Align test set columns with training set columns\n    test_features = test_df[feature_columns]\n\n    model = FuelBlendingPredictor(use_gpu=True)\n    model.fit(train_df[feature_columns], train_df[target_columns])\n    \n    predictions = model.predict(test_features)\n    \n    submission_id_col = 'ID' if 'ID' in test_df.columns else test_df.index.name\n    if submission_id_col not in test_df:\n        submission = pd.DataFrame({'ID': test_df.index})\n    else:\n        submission = pd.DataFrame({'ID': test_df[submission_id_col]})\n        \n    submission = pd.concat([submission, predictions], axis=1)\n        \n    submission.to_csv('submission.csv', index=False)\n    print(\"\\n💾 Submission file 'submission.csv' saved successfully.\")\n    print(submission.head())\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T06:56:50.263914Z","iopub.execute_input":"2025-07-19T06:56:50.264587Z","iopub.status.idle":"2025-07-19T07:02:10.144193Z","shell.execute_reply.started":"2025-07-19T06:56:50.264564Z","shell.execute_reply":"2025-07-19T07:02:10.143226Z"}},"outputs":[{"name":"stdout","text":"CuPy found. GPU data acceleration is enabled.\nUsing PyTorch device: cuda\n🚀 Starting Fuel Blending ML Pipeline\nSelected 57 features out of 57\nTraining Neural Network...\nEarly stopping at epoch 174\nNeural Network training completed. Best validation loss: 0.065875\nTraining Gradient Boosting Models...\nTraining xgb with per-target early stopping...\nxgb training completed in 18.04s.\nTraining catboost with per-target early stopping...\ncatboost training completed in 232.27s.\nTraining lgb with per-target early stopping...\n","output_type":"stream"},{"name":"stderr","text":"1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n1 warning generated.\n","output_type":"stream"},{"name":"stdout","text":"lgb training completed in 26.01s.\nTraining rf...\nrf training completed in 21.61s.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mCatBoostError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/248964035.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/248964035.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFuelBlendingPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/248964035.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_gradient_boosting_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m         \u001b[0mval_preds_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_ensemble_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         \u001b[0mval_preds_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_scaler'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_preds_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0my_val_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target_scaler'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/248964035.py\u001b[0m in \u001b[0;36mcreate_ensemble_predictions\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0mis_gpu_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'xgb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lgb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'catboost'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0mpred_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_device\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_cupy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_gpu_model\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred_device\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_cupy\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpred_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/248964035.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/248964035.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[1;32m   5922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprediction_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5923\u001b[0m             \u001b[0mprediction_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_default_prediction_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5924\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5926\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstaged_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RawFormulaVal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntree_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_period\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n\u001b[1;32m   2618\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2619\u001b[0m             \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2620\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_is_single_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_predict_input_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_method_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_prediction_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_process_predict_input_data\u001b[0;34m(self, data, parent_method_name, thread_count, label)\u001b[0m\n\u001b[1;32m   2596\u001b[0m             raise CatBoostError((\"There is no trained model to use {}(). \"\n\u001b[1;32m   2597\u001b[0m                                  \"Use fit() to train model. Then use this method.\").format(parent_method_name))\n\u001b[0;32m-> 2598\u001b[0;31m         \u001b[0mis_single_object\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_is_data_single_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2599\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2600\u001b[0m             data = Pool(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/catboost/core.py\u001b[0m in \u001b[0;36m_is_data_single_object\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2172\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mARRAY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2174\u001b[0;31m         raise CatBoostError(\n\u001b[0m\u001b[1;32m   2175\u001b[0m             \u001b[0;34m\"Invalid data type={} : must be list, numpy.ndarray, pandas.Series, pandas.DataFrame,\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2176\u001b[0m             \u001b[0;34m\" scipy.sparse matrix, catboost.FeaturesData or catboost.Pool\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mCatBoostError\u001b[0m: Invalid data type=<class 'cupy.ndarray'> : must be list, numpy.ndarray, pandas.Series, pandas.DataFrame, scipy.sparse matrix, catboost.FeaturesData or catboost.Pool"],"ename":"CatBoostError","evalue":"Invalid data type=<class 'cupy.ndarray'> : must be list, numpy.ndarray, pandas.Series, pandas.DataFrame, scipy.sparse matrix, catboost.FeaturesData or catboost.Pool","output_type":"error"}],"execution_count":11}]}