{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12405754,"sourceType":"datasetVersion","datasetId":7823452},{"sourceId":12406847,"sourceType":"datasetVersion","datasetId":7824237},{"sourceId":12513943,"sourceType":"datasetVersion","datasetId":7898717}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0a4bf7ee","cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, Model, regularizers, optimizers, callbacks\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\nfrom sklearn.multioutput import MultiOutputRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\nimport catboost as cb\nimport warnings\nimport time\nimport abc\nimport os\n\n# --- Environment Setup ---\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nwarnings.filterwarnings('ignore')\n\ntry:\n    import cupy as cp\n    CUPY_AVAILABLE = True\n    print(\"CuPy found. GPU data acceleration is enabled.\")\nexcept ImportError:\n    CUPY_AVAILABLE = False\n    print(\"CuPy not found. Predictions will use CPU data, which may be slower.\")\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"TensorFlow is using GPU: {gpus[0].name}\")\n        TF_DEVICE = \"/GPU:0\"\n    except RuntimeError as e:\n        print(e); TF_DEVICE = \"/CPU:0\"\nelse:\n    print(\"TensorFlow is using CPU.\"); TF_DEVICE = \"/CPU:0\"\n\n# =========================================================================\n# Helper and Base Classes\n# =========================================================================\nclass MultiTargetModelWrapper:\n    def __init__(self, models): self.models = models\n    def predict(self, X):\n        # This wrapper's predict now expects a NumPy array because of the fix below\n        preds = [model.predict(X) for model in self.models]\n        return np.column_stack(preds)\n\nclass BasePredictor(abc.ABC):\n    def __init__(self, use_gpu=True):\n        self.model = None; self.scalers = {}; self.best_features = None\n        self.target_names = None; self.is_fitted = False\n        self.device_type = 'cuda' if use_gpu and tf.config.list_physical_devices('GPU') else 'cpu'\n    def create_advanced_features(self, df):\n        df_features = df.copy()\n        blend_cols = [col for col in df.columns if 'fraction' in col]\n        if not blend_cols: return df_features\n        df_features['blend_entropy'] = -np.sum(df[blend_cols] * np.log(df[blend_cols] + 1e-8), axis=1)\n        return df_features\n    def select_best_features(self, X, y, max_features=150):\n        X = X.fillna(0)\n        scores = pd.Series(mutual_info_regression(X, y.mean(axis=1), random_state=42), index=X.columns)\n        return scores.nlargest(max_features).index.tolist()\n    def fit(self, X, y):\n        print(f\"--- Fitting {self.__class__.__name__} ---\")\n        start_time = time.time()\n        self.target_names = y.columns.tolist()\n        X_processed = self.create_advanced_features(X)\n        self.best_features = self.select_best_features(X_processed, y)\n        X_selected = X_processed[self.best_features]\n        self.scalers['feature_scaler'] = RobustScaler()\n        X_scaled = pd.DataFrame(self.scalers['feature_scaler'].fit_transform(X_selected), columns=self.best_features)\n        self.scalers['target_scaler'] = RobustScaler()\n        y_scaled = pd.DataFrame(self.scalers['target_scaler'].fit_transform(y), columns=y.columns)\n        X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n        self.model = self._train_model(X_train, y_train, X_val, y_val)\n        self.is_fitted = True\n        val_preds_scaled = self._predict_model(X_val)\n        val_preds_orig = self.scalers['target_scaler'].inverse_transform(val_preds_scaled)\n        y_val_orig = self.scalers['target_scaler'].inverse_transform(y_val)\n        mape = mean_absolute_percentage_error(y_val_orig, val_preds_orig)\n        print(f\"Validation MAPE for {self.__class__.__name__}: {mape:.6f}\")\n        print(f\"Completed fitting in {time.time() - start_time:.2f}s\\n\")\n        return self\n    def predict(self, X):\n        if not self.is_fitted: raise RuntimeError(\"Must fit before predicting.\")\n        X_processed = self.create_advanced_features(X)\n        X_selected = X_processed[self.best_features]\n        X_scaled = pd.DataFrame(self.scalers['feature_scaler'].transform(X_selected), columns=self.best_features)\n        preds_scaled = self._predict_model(X_scaled)\n        preds_orig = self.scalers['target_scaler'].inverse_transform(preds_scaled)\n        return pd.DataFrame(preds_orig, columns=self.target_names)\n    @abc.abstractmethod\n    def _train_model(self, X_train, y_train, X_val, y_val): pass\n    @abc.abstractmethod\n    def _predict_model(self, X): pass\n\n# =========================================================================\n# Model Implementations\n# =========================================================================\n\nclass TensorFlowPredictor(BasePredictor):\n    class ResidualBlock(layers.Layer):\n        def __init__(self, units, dropout_rate, l2_reg, **kwargs):\n            super().__init__(**kwargs)\n            self.main_layers = [\n                layers.Dense(units, activation='gelu', kernel_regularizer=regularizers.l2(l2_reg)),\n                layers.BatchNormalization(), layers.Dropout(dropout_rate)\n            ]\n            self.skip_layers = [layers.Dense(units), layers.BatchNormalization()]\n        def call(self, inputs):\n            Z = inputs; Z = self.main_layers[0](Z); Z = self.main_layers[1](Z); Z = self.main_layers[2](Z)\n            skip_Z = inputs; skip_Z = self.skip_layers[0](skip_Z); skip_Z = self.skip_layers[1](skip_Z)\n            return layers.add([Z, skip_Z])\n    class FeatureAttention(layers.Layer):\n        def build(self, input_shape):\n            num_features = input_shape[-1]\n            self.dense1 = layers.Dense(max(1, num_features // 4), activation='tanh', name=\"attention_context\")\n            self.dense2 = layers.Dense(num_features, activation='softmax', name=\"attention_weights\")\n            super().build(input_shape)\n        def call(self, inputs):\n            context = self.dense1(inputs)\n            attention_weights = self.dense2(context)\n            return inputs * attention_weights\n    def _create_model(self, input_shape, output_shape, wide_input_shape):\n        l2_reg = 1e-5; dropout_rate = 0.2\n        residual_units = [512, 512, 256, 256, 128]\n        deep_input = layers.Input(shape=(input_shape,), name='deep_input')\n        Z = deep_input\n        for units in residual_units: Z = self.ResidualBlock(units, dropout_rate, l2_reg)(Z)\n        Z = self.FeatureAttention()(Z)\n        wide_input = layers.Input(shape=(wide_input_shape,), name='wide_input')\n        combined = layers.concatenate([Z, wide_input])\n        output = layers.Dense(256, activation='gelu')(combined)\n        output = layers.BatchNormalization()(output)\n        output = layers.Dropout(dropout_rate)(output)\n        output = layers.Dense(output_shape, name='output')(output)\n        return Model(inputs=[deep_input, wide_input], outputs=output)\n    def _train_model(self, X_train, y_train, X_val, y_val):\n        with tf.device(TF_DEVICE):\n            input_shape = X_train.shape[1]\n            wide_feature_count = max(1, input_shape // 10)\n            model = self._create_model(input_shape, y_train.shape[1], wide_input_shape=wide_feature_count)\n            optimizer = optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5)\n            model.compile(loss='mean_squared_error', optimizer=optimizer)\n            callbacks_list = [\n                callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n                callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=15)\n            ]\n            X_train_deep, X_train_wide = X_train.values, X_train.values[:, :wide_feature_count]\n            X_val_deep, X_val_wide = X_val.values, X_val.values[:, :wide_feature_count]\n            model.fit([X_train_deep, X_train_wide], y_train, validation_data=([X_val_deep, X_val_wide], y_val),\n                      epochs=500, batch_size=64, callbacks=callbacks_list, verbose=1)\n        return model\n    def _predict_model(self, X):\n        with tf.device(TF_DEVICE):\n            wide_feature_count = max(1, X.shape[1] // 10)\n            X_deep, X_wide = X.values, X.values[:, :wide_feature_count]\n            return self.model.predict([X_deep, X_wide], verbose=0)\n\nclass XGBoostPredictor(BasePredictor):\n    def _train_model(self, X_train, y_train, X_val, y_val):\n        params = {'n_estimators': 2000, 'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.8,\n                  'colsample_bytree': 0.8, 'random_state': 42, 'tree_method': 'hist',\n                  'device': 'cuda' if self.device_type == 'cuda' else 'cpu', 'early_stopping_rounds': 50}\n        models = [xgb.XGBRegressor(**params).fit(X_train, y_train.iloc[:, i], eval_set=[(X_val, y_val.iloc[:, i])], verbose=False) for i in range(y_train.shape[1])]\n        return MultiTargetModelWrapper(models)\n    def _predict_model(self, X):\n        # ### THIS IS THE FIX ###\n        # If X is a CuPy array, convert it to NumPy before prediction.\n        if CUPY_AVAILABLE and isinstance(X, cp.ndarray):\n            X = X.get()\n        return self.model.predict(X)\n\nclass LightGBMPredictor(BasePredictor):\n    def _train_model(self, X_train, y_train, X_val, y_val):\n        params = {'n_estimators': 2500, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 8,\n                  'max_bin': 128, 'random_state': 42,\n                  'device': 'gpu' if self.device_type == 'cuda' else 'cpu', 'verbose': -1}\n        models = [lgb.LGBMRegressor(**params).fit(X_train, y_train.iloc[:, i], eval_set=[(X_val, y_val.iloc[:, i])], callbacks=[lgb.early_stopping(50, verbose=False)]) for i in range(y_train.shape[1])]\n        return MultiTargetModelWrapper(models)\n    def _predict_model(self, X):\n        # ### THIS IS THE FIX ###\n        # If X is a CuPy array, convert it to NumPy before prediction.\n        if CUPY_AVAILABLE and isinstance(X, cp.ndarray):\n            X = X.get()\n        return self.model.predict(X)\n\nclass CatBoostPredictor(BasePredictor):\n    def _train_model(self, X_train, y_train, X_val, y_val):\n        params = {'iterations': 2000, 'learning_rate': 0.05, 'depth': 7, 'random_seed': 42,\n                  'task_type': 'GPU' if self.device_type == 'cuda' else 'CPU',\n                  'verbose': 0, 'allow_writing_files': False, 'early_stopping_rounds': 50}\n        models = [cb.CatBoostRegressor(**params).fit(X_train, y_train.iloc[:, i], eval_set=[(X_val, y_val.iloc[:, i])], verbose=False) for i in range(y_train.shape[1])]\n        return MultiTargetModelWrapper(models)\n    def _predict_model(self, X):\n        # ### THIS IS THE FIX ###\n        # If X is a CuPy array, convert it to NumPy before prediction.\n        if CUPY_AVAILABLE and isinstance(X, cp.ndarray):\n            X = X.get()\n        return self.model.predict(X)\n\nclass RandomForestPredictor(BasePredictor):\n    def _train_model(self, X_train, y_train, X_val, y_val):\n        model = RandomForestRegressor(n_estimators=150, max_depth=12, min_samples_leaf=3, random_state=42, n_jobs=-1)\n        return MultiOutputRegressor(model, n_jobs=-1).fit(X_train, y_train)\n    def _predict_model(self, X): return self.model.predict(X)\n\n# =========================================================================\n# Ensemble Predictor\n# =========================================================================\nclass EnsemblePredictor:\n    def __init__(self, models, weights):\n        if not models or sum(weights) == 0: raise ValueError(\"Models and weights required.\")\n        self.models = models\n        self.weights = np.array(weights) / np.sum(weights)\n        self.target_names = None\n    def fit(self, X, y):\n        print(\"====== STARTING ENSEMBLE TRAINING ======\")\n        self.target_names = y.columns.tolist()\n        for model in self.models:\n            model.fit(X.copy(), y.copy())\n        print(\"====== ENSEMBLE TRAINING COMPLETE ======\")\n        return self\n    def predict(self, X):\n        print(\"\\n====== GENERATING ENSEMBLE PREDICTIONS ======\")\n        all_preds = [m.predict(X.copy()) for m in self.models]\n        all_preds_np = [p.values if isinstance(p, pd.DataFrame) else p for p in all_preds]\n        ensembled = np.average(all_preds_np, axis=0, weights=self.weights)\n        return pd.DataFrame(ensembled, columns=self.target_names)\n\n# =========================================================================\n# Main Execution Block\n# =========================================================================\ndef main():\n    print(\"ğŸš€ Starting Fuel Blending ML Pipeline\")\n    try:\n        train_df = pd.read_csv('/kaggle/input/training/train.csv')\n        test_df = pd.read_csv('/kaggle/input/testing/test.csv')\n    except FileNotFoundError as e:\n        print(f\"Error loading data: {e}. Please check file paths.\")\n        return\n    \n    target_columns = [col for col in train_df.columns if 'BlendProperty' in col]\n    feature_columns = [col for col in train_df.columns if col not in target_columns and 'ID' not in col]\n    \n    X_train, y_train = train_df[feature_columns], train_df[target_columns]\n    X_test = test_df[feature_columns]\n\n    models_to_train = [\n        TensorFlowPredictor(),\n        XGBoostPredictor(),\n        LightGBMPredictor(),\n        CatBoostPredictor(),\n        RandomForestPredictor()\n    ]\n    ensemble_weights = [0.40, 0.20, 0.20, 0.15, 0.05]\n\n    ensemble_model = EnsemblePredictor(models=models_to_train, weights=ensemble_weights)\n    ensemble_model.fit(X_train, y_train)\n\n    predictions = ensemble_model.predict(X_test)\n    submission = pd.DataFrame({'ID': test_df.get('ID', test_df.index)})\n    submission = pd.concat([submission, predictions], axis=1)\n    submission.to_csv('submission.csv', index=False)\n    \n    print(\"\\nğŸ’¾ Submission file 'submission.csv' saved successfully.\")\n    print(submission.head())\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T07:34:23.160902Z","iopub.execute_input":"2025-07-19T07:34:23.161535Z","iopub.status.idle":"2025-07-19T07:40:25.772030Z","shell.execute_reply.started":"2025-07-19T07:34:23.161512Z","shell.execute_reply":"2025-07-19T07:40:25.771237Z"}},"outputs":[{"name":"stdout","text":"CuPy found. GPU data acceleration is enabled.\nTensorFlow is using GPU: /physical_device:GPU:0\nğŸš€ Starting Fuel Blending ML Pipeline\n====== STARTING ENSEMBLE TRAINING ======\n--- Fitting TensorFlowPredictor ---\nEpoch 1/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 71ms/step - loss: 1.3492 - val_loss: 0.5099 - learning_rate: 0.0010\nEpoch 2/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4994 - val_loss: 0.5015 - learning_rate: 0.0010\nEpoch 3/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.4254 - val_loss: 0.4966 - learning_rate: 0.0010\nEpoch 4/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3872 - val_loss: 0.4896 - learning_rate: 0.0010\nEpoch 5/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3551 - val_loss: 0.4818 - learning_rate: 0.0010\nEpoch 6/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3224 - val_loss: 0.4767 - learning_rate: 0.0010\nEpoch 7/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2935 - val_loss: 0.4627 - learning_rate: 0.0010\nEpoch 8/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2610 - val_loss: 0.4553 - learning_rate: 0.0010\nEpoch 9/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2441 - val_loss: 0.4440 - learning_rate: 0.0010\nEpoch 10/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2139 - val_loss: 0.4283 - learning_rate: 0.0010\nEpoch 11/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2020 - val_loss: 0.4187 - learning_rate: 0.0010\nEpoch 12/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1841 - val_loss: 0.3996 - learning_rate: 0.0010\nEpoch 13/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1620 - val_loss: 0.3787 - learning_rate: 0.0010\nEpoch 14/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1503 - val_loss: 0.3671 - learning_rate: 0.0010\nEpoch 15/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1450 - val_loss: 0.3421 - learning_rate: 0.0010\nEpoch 16/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1336 - val_loss: 0.3295 - learning_rate: 0.0010\nEpoch 17/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1220 - val_loss: 0.3121 - learning_rate: 0.0010\nEpoch 18/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1122 - val_loss: 0.2916 - learning_rate: 0.0010\nEpoch 19/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1058 - val_loss: 0.2701 - learning_rate: 0.0010\nEpoch 20/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0980 - val_loss: 0.2425 - learning_rate: 0.0010\nEpoch 21/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0910 - val_loss: 0.2165 - learning_rate: 0.0010\nEpoch 22/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0886 - val_loss: 0.2013 - learning_rate: 0.0010\nEpoch 23/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0887 - val_loss: 0.1859 - learning_rate: 0.0010\nEpoch 24/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0836 - val_loss: 0.1666 - learning_rate: 0.0010\nEpoch 25/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0760 - val_loss: 0.1400 - learning_rate: 0.0010\nEpoch 26/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0760 - val_loss: 0.1316 - learning_rate: 0.0010\nEpoch 27/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0735 - val_loss: 0.1220 - learning_rate: 0.0010\nEpoch 28/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0711 - val_loss: 0.1002 - learning_rate: 0.0010\nEpoch 29/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0650 - val_loss: 0.0987 - learning_rate: 0.0010\nEpoch 30/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0658 - val_loss: 0.0869 - learning_rate: 0.0010\nEpoch 31/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0645 - val_loss: 0.0820 - learning_rate: 0.0010\nEpoch 32/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0607 - val_loss: 0.0804 - learning_rate: 0.0010\nEpoch 33/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0649 - val_loss: 0.0764 - learning_rate: 0.0010\nEpoch 34/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0593 - val_loss: 0.0791 - learning_rate: 0.0010\nEpoch 35/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0588 - val_loss: 0.0703 - learning_rate: 0.0010\nEpoch 36/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0566 - val_loss: 0.0694 - learning_rate: 0.0010\nEpoch 37/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0600 - val_loss: 0.0698 - learning_rate: 0.0010\nEpoch 38/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0569 - val_loss: 0.0682 - learning_rate: 0.0010\nEpoch 39/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0554 - val_loss: 0.0680 - learning_rate: 0.0010\nEpoch 40/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0537 - val_loss: 0.0640 - learning_rate: 0.0010\nEpoch 41/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0519 - val_loss: 0.0667 - learning_rate: 0.0010\nEpoch 42/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0550 - val_loss: 0.0656 - learning_rate: 0.0010\nEpoch 43/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0501 - val_loss: 0.0665 - learning_rate: 0.0010\nEpoch 44/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0503 - val_loss: 0.0616 - learning_rate: 0.0010\nEpoch 45/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0468 - val_loss: 0.0622 - learning_rate: 0.0010\nEpoch 46/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0491 - val_loss: 0.0616 - learning_rate: 0.0010\nEpoch 47/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0469 - val_loss: 0.0654 - learning_rate: 0.0010\nEpoch 48/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0483 - val_loss: 0.0619 - learning_rate: 0.0010\nEpoch 49/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0459 - val_loss: 0.0595 - learning_rate: 0.0010\nEpoch 50/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0491 - val_loss: 0.0660 - learning_rate: 0.0010\nEpoch 51/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0446 - val_loss: 0.0600 - learning_rate: 0.0010\nEpoch 52/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0451 - val_loss: 0.0620 - learning_rate: 0.0010\nEpoch 53/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0460 - val_loss: 0.0586 - learning_rate: 0.0010\nEpoch 54/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0452 - val_loss: 0.0592 - learning_rate: 0.0010\nEpoch 55/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0442 - val_loss: 0.0579 - learning_rate: 0.0010\nEpoch 56/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0434 - val_loss: 0.0604 - learning_rate: 0.0010\nEpoch 57/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0458 - val_loss: 0.0581 - learning_rate: 0.0010\nEpoch 58/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0433 - val_loss: 0.0589 - learning_rate: 0.0010\nEpoch 59/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0461 - val_loss: 0.0548 - learning_rate: 0.0010\nEpoch 60/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0454 - val_loss: 0.0563 - learning_rate: 0.0010\nEpoch 61/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0461 - val_loss: 0.0580 - learning_rate: 0.0010\nEpoch 62/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0453 - val_loss: 0.0599 - learning_rate: 0.0010\nEpoch 63/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0426 - val_loss: 0.0583 - learning_rate: 0.0010\nEpoch 64/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0440 - val_loss: 0.0553 - learning_rate: 0.0010\nEpoch 65/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0398 - val_loss: 0.0583 - learning_rate: 0.0010\nEpoch 66/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0440 - val_loss: 0.0610 - learning_rate: 0.0010\nEpoch 67/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0424 - val_loss: 0.0572 - learning_rate: 0.0010\nEpoch 68/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0416 - val_loss: 0.0595 - learning_rate: 0.0010\nEpoch 69/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0432 - val_loss: 0.0543 - learning_rate: 0.0010\nEpoch 70/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0419 - val_loss: 0.0581 - learning_rate: 0.0010\nEpoch 71/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0411 - val_loss: 0.0561 - learning_rate: 0.0010\nEpoch 72/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0435 - val_loss: 0.0622 - learning_rate: 0.0010\nEpoch 73/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0454 - val_loss: 0.0552 - learning_rate: 0.0010\nEpoch 74/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0416 - val_loss: 0.0570 - learning_rate: 0.0010\nEpoch 75/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0454 - val_loss: 0.0584 - learning_rate: 0.0010\nEpoch 76/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0381 - val_loss: 0.0557 - learning_rate: 0.0010\nEpoch 77/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0420 - val_loss: 0.0557 - learning_rate: 0.0010\nEpoch 78/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0367 - val_loss: 0.0552 - learning_rate: 0.0010\nEpoch 79/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0396 - val_loss: 0.0578 - learning_rate: 0.0010\nEpoch 80/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0398 - val_loss: 0.0578 - learning_rate: 0.0010\nEpoch 81/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0399 - val_loss: 0.0669 - learning_rate: 0.0010\nEpoch 82/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0425 - val_loss: 0.0622 - learning_rate: 0.0010\nEpoch 83/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0368 - val_loss: 0.0565 - learning_rate: 0.0010\nEpoch 84/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0394 - val_loss: 0.0560 - learning_rate: 0.0010\nEpoch 85/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0411 - val_loss: 0.0530 - learning_rate: 5.0000e-04\nEpoch 86/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0332 - val_loss: 0.0523 - learning_rate: 5.0000e-04\nEpoch 87/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0395 - val_loss: 0.0531 - learning_rate: 5.0000e-04\nEpoch 88/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0350 - val_loss: 0.0541 - learning_rate: 5.0000e-04\nEpoch 89/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0351 - val_loss: 0.0525 - learning_rate: 5.0000e-04\nEpoch 90/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0371 - val_loss: 0.0517 - learning_rate: 5.0000e-04\nEpoch 91/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0382 - val_loss: 0.0528 - learning_rate: 5.0000e-04\nEpoch 92/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0329 - val_loss: 0.0534 - learning_rate: 5.0000e-04\nEpoch 93/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0342 - val_loss: 0.0528 - learning_rate: 5.0000e-04\nEpoch 94/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0357 - val_loss: 0.0529 - learning_rate: 5.0000e-04\nEpoch 95/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0316 - val_loss: 0.0516 - learning_rate: 5.0000e-04\nEpoch 96/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0340 - val_loss: 0.0520 - learning_rate: 5.0000e-04\nEpoch 97/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0381 - val_loss: 0.0522 - learning_rate: 5.0000e-04\nEpoch 98/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0340 - val_loss: 0.0526 - learning_rate: 5.0000e-04\nEpoch 99/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0329 - val_loss: 0.0516 - learning_rate: 5.0000e-04\nEpoch 100/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0326 - val_loss: 0.0519 - learning_rate: 5.0000e-04\nEpoch 101/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0298 - val_loss: 0.0516 - learning_rate: 5.0000e-04\nEpoch 102/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0354 - val_loss: 0.0519 - learning_rate: 5.0000e-04\nEpoch 103/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0371 - val_loss: 0.0519 - learning_rate: 5.0000e-04\nEpoch 104/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0319 - val_loss: 0.0526 - learning_rate: 5.0000e-04\nEpoch 105/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0343 - val_loss: 0.0535 - learning_rate: 5.0000e-04\nEpoch 106/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0330 - val_loss: 0.0515 - learning_rate: 5.0000e-04\nEpoch 107/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0325 - val_loss: 0.0539 - learning_rate: 5.0000e-04\nEpoch 108/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0345 - val_loss: 0.0524 - learning_rate: 5.0000e-04\nEpoch 109/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0318 - val_loss: 0.0546 - learning_rate: 5.0000e-04\nEpoch 110/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0331 - val_loss: 0.0507 - learning_rate: 5.0000e-04\nEpoch 111/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0326 - val_loss: 0.0494 - learning_rate: 5.0000e-04\nEpoch 112/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0318 - val_loss: 0.0494 - learning_rate: 5.0000e-04\nEpoch 113/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0308 - val_loss: 0.0503 - learning_rate: 5.0000e-04\nEpoch 114/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0348 - val_loss: 0.0511 - learning_rate: 5.0000e-04\nEpoch 115/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0298 - val_loss: 0.0523 - learning_rate: 5.0000e-04\nEpoch 116/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0357 - val_loss: 0.0515 - learning_rate: 5.0000e-04\nEpoch 117/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0313 - val_loss: 0.0496 - learning_rate: 5.0000e-04\nEpoch 118/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0334 - val_loss: 0.0515 - learning_rate: 5.0000e-04\nEpoch 119/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0307 - val_loss: 0.0526 - learning_rate: 5.0000e-04\nEpoch 120/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0315 - val_loss: 0.0488 - learning_rate: 5.0000e-04\nEpoch 121/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0306 - val_loss: 0.0488 - learning_rate: 5.0000e-04\nEpoch 122/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0353 - val_loss: 0.0503 - learning_rate: 5.0000e-04\nEpoch 123/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0297 - val_loss: 0.0507 - learning_rate: 5.0000e-04\nEpoch 124/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0318 - val_loss: 0.0458 - learning_rate: 5.0000e-04\nEpoch 125/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0318 - val_loss: 0.0466 - learning_rate: 5.0000e-04\nEpoch 126/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0325 - val_loss: 0.0481 - learning_rate: 5.0000e-04\nEpoch 127/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0305 - val_loss: 0.0494 - learning_rate: 5.0000e-04\nEpoch 128/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0328 - val_loss: 0.0483 - learning_rate: 5.0000e-04\nEpoch 129/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0290 - val_loss: 0.0482 - learning_rate: 5.0000e-04\nEpoch 130/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0319 - val_loss: 0.0511 - learning_rate: 5.0000e-04\nEpoch 131/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0307 - val_loss: 0.0503 - learning_rate: 5.0000e-04\nEpoch 132/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0303 - val_loss: 0.0507 - learning_rate: 5.0000e-04\nEpoch 133/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0303 - val_loss: 0.0492 - learning_rate: 5.0000e-04\nEpoch 134/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0304 - val_loss: 0.0470 - learning_rate: 5.0000e-04\nEpoch 135/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0289 - val_loss: 0.0480 - learning_rate: 5.0000e-04\nEpoch 136/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0297 - val_loss: 0.0500 - learning_rate: 5.0000e-04\nEpoch 137/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0288 - val_loss: 0.0477 - learning_rate: 5.0000e-04\nEpoch 138/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0370 - val_loss: 0.0480 - learning_rate: 5.0000e-04\nEpoch 139/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0299 - val_loss: 0.0509 - learning_rate: 5.0000e-04\nEpoch 140/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0324 - val_loss: 0.0464 - learning_rate: 2.5000e-04\nEpoch 141/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0329 - val_loss: 0.0461 - learning_rate: 2.5000e-04\nEpoch 142/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0301 - val_loss: 0.0463 - learning_rate: 2.5000e-04\nEpoch 143/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0262 - val_loss: 0.0459 - learning_rate: 2.5000e-04\nEpoch 144/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0301 - val_loss: 0.0454 - learning_rate: 2.5000e-04\nEpoch 145/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0275 - val_loss: 0.0464 - learning_rate: 2.5000e-04\nEpoch 146/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0269 - val_loss: 0.0464 - learning_rate: 2.5000e-04\nEpoch 147/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0291 - val_loss: 0.0473 - learning_rate: 2.5000e-04\nEpoch 148/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0310 - val_loss: 0.0479 - learning_rate: 2.5000e-04\nEpoch 149/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0301 - val_loss: 0.0476 - learning_rate: 2.5000e-04\nEpoch 150/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0276 - val_loss: 0.0484 - learning_rate: 2.5000e-04\nEpoch 151/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0286 - val_loss: 0.0482 - learning_rate: 2.5000e-04\nEpoch 152/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0277 - val_loss: 0.0461 - learning_rate: 2.5000e-04\nEpoch 153/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0279 - val_loss: 0.0473 - learning_rate: 2.5000e-04\nEpoch 154/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0255 - val_loss: 0.0458 - learning_rate: 2.5000e-04\nEpoch 155/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0271 - val_loss: 0.0474 - learning_rate: 2.5000e-04\nEpoch 156/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0276 - val_loss: 0.0461 - learning_rate: 2.5000e-04\nEpoch 157/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0262 - val_loss: 0.0462 - learning_rate: 2.5000e-04\nEpoch 158/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0262 - val_loss: 0.0471 - learning_rate: 2.5000e-04\nEpoch 159/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0263 - val_loss: 0.0462 - learning_rate: 2.5000e-04\nEpoch 160/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0296 - val_loss: 0.0462 - learning_rate: 1.2500e-04\nEpoch 161/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0278 - val_loss: 0.0450 - learning_rate: 1.2500e-04\nEpoch 162/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0291 - val_loss: 0.0463 - learning_rate: 1.2500e-04\nEpoch 163/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0284 - val_loss: 0.0467 - learning_rate: 1.2500e-04\nEpoch 164/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0277 - val_loss: 0.0459 - learning_rate: 1.2500e-04\nEpoch 165/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0265 - val_loss: 0.0461 - learning_rate: 1.2500e-04\nEpoch 166/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0276 - val_loss: 0.0454 - learning_rate: 1.2500e-04\nEpoch 167/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0287 - val_loss: 0.0460 - learning_rate: 1.2500e-04\nEpoch 168/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0280 - val_loss: 0.0469 - learning_rate: 1.2500e-04\nEpoch 169/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0263 - val_loss: 0.0485 - learning_rate: 1.2500e-04\nEpoch 170/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0285 - val_loss: 0.0486 - learning_rate: 1.2500e-04\nEpoch 171/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0254 - val_loss: 0.0468 - learning_rate: 1.2500e-04\nEpoch 172/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0249 - val_loss: 0.0457 - learning_rate: 1.2500e-04\nEpoch 173/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0286 - val_loss: 0.0451 - learning_rate: 1.2500e-04\nEpoch 174/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0269 - val_loss: 0.0460 - learning_rate: 1.2500e-04\nEpoch 175/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0259 - val_loss: 0.0462 - learning_rate: 1.2500e-04\nEpoch 176/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0267 - val_loss: 0.0466 - learning_rate: 1.2500e-04\nEpoch 177/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0297 - val_loss: 0.0456 - learning_rate: 6.2500e-05\nEpoch 178/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0272 - val_loss: 0.0456 - learning_rate: 6.2500e-05\nEpoch 179/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0240 - val_loss: 0.0447 - learning_rate: 6.2500e-05\nEpoch 180/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0241 - val_loss: 0.0444 - learning_rate: 6.2500e-05\nEpoch 181/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0278 - val_loss: 0.0445 - learning_rate: 6.2500e-05\nEpoch 182/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0257 - val_loss: 0.0446 - learning_rate: 6.2500e-05\nEpoch 183/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0267 - val_loss: 0.0449 - learning_rate: 6.2500e-05\nEpoch 184/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0271 - val_loss: 0.0443 - learning_rate: 6.2500e-05\nEpoch 185/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0271 - val_loss: 0.0440 - learning_rate: 6.2500e-05\nEpoch 186/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0265 - val_loss: 0.0444 - learning_rate: 6.2500e-05\nEpoch 187/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0265 - val_loss: 0.0436 - learning_rate: 6.2500e-05\nEpoch 188/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0262 - val_loss: 0.0441 - learning_rate: 6.2500e-05\nEpoch 189/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0242 - val_loss: 0.0441 - learning_rate: 6.2500e-05\nEpoch 190/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0273 - val_loss: 0.0446 - learning_rate: 6.2500e-05\nEpoch 191/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0270 - val_loss: 0.0454 - learning_rate: 6.2500e-05\nEpoch 192/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0255 - val_loss: 0.0449 - learning_rate: 6.2500e-05\nEpoch 193/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0273 - val_loss: 0.0454 - learning_rate: 6.2500e-05\nEpoch 194/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0252 - val_loss: 0.0452 - learning_rate: 6.2500e-05\nEpoch 195/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0310 - val_loss: 0.0457 - learning_rate: 6.2500e-05\nEpoch 196/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0268 - val_loss: 0.0456 - learning_rate: 6.2500e-05\nEpoch 197/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0250 - val_loss: 0.0453 - learning_rate: 6.2500e-05\nEpoch 198/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0250 - val_loss: 0.0450 - learning_rate: 6.2500e-05\nEpoch 199/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0271 - val_loss: 0.0450 - learning_rate: 6.2500e-05\nEpoch 200/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0257 - val_loss: 0.0455 - learning_rate: 6.2500e-05\nEpoch 201/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0258 - val_loss: 0.0451 - learning_rate: 6.2500e-05\nEpoch 202/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0253 - val_loss: 0.0447 - learning_rate: 6.2500e-05\nEpoch 203/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0246 - val_loss: 0.0445 - learning_rate: 3.1250e-05\nEpoch 204/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0258 - val_loss: 0.0445 - learning_rate: 3.1250e-05\nEpoch 205/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0242 - val_loss: 0.0446 - learning_rate: 3.1250e-05\nEpoch 206/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0250 - val_loss: 0.0447 - learning_rate: 3.1250e-05\nEpoch 207/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0258 - val_loss: 0.0446 - learning_rate: 3.1250e-05\nEpoch 208/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0260 - val_loss: 0.0443 - learning_rate: 3.1250e-05\nEpoch 209/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0281 - val_loss: 0.0443 - learning_rate: 3.1250e-05\nEpoch 210/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0244 - val_loss: 0.0445 - learning_rate: 3.1250e-05\nEpoch 211/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0265 - val_loss: 0.0448 - learning_rate: 3.1250e-05\nEpoch 212/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0289 - val_loss: 0.0447 - learning_rate: 3.1250e-05\nEpoch 213/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0235 - val_loss: 0.0442 - learning_rate: 3.1250e-05\nEpoch 214/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0249 - val_loss: 0.0440 - learning_rate: 3.1250e-05\nEpoch 215/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0241 - val_loss: 0.0440 - learning_rate: 3.1250e-05\nEpoch 216/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0308 - val_loss: 0.0439 - learning_rate: 3.1250e-05\nEpoch 217/500\n\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0279 - val_loss: 0.0438 - learning_rate: 3.1250e-05\nValidation MAPE for TensorFlowPredictor: 2.157897\nCompleted fitting in 66.90s\n\n--- Fitting XGBoostPredictor ---\nValidation MAPE for XGBoostPredictor: 2.793977\nCompleted fitting in 19.79s\n\n--- Fitting LightGBMPredictor ---\nValidation MAPE for LightGBMPredictor: 1.947443\nCompleted fitting in 19.85s\n\n--- Fitting CatBoostPredictor ---\nValidation MAPE for CatBoostPredictor: 1.303359\nCompleted fitting in 231.74s\n\n--- Fitting RandomForestPredictor ---\nValidation MAPE for RandomForestPredictor: 4.775212\nCompleted fitting in 22.72s\n\n====== ENSEMBLE TRAINING COMPLETE ======\n\n====== GENERATING ENSEMBLE PREDICTIONS ======\n\nğŸ’¾ Submission file 'submission.csv' saved successfully.\n   ID  BlendProperty1  BlendProperty2  BlendProperty3  BlendProperty4  \\\n0   1        0.058428        0.178983        0.589842        0.437011   \n1   2       -0.510171       -0.690518       -1.164707       -0.055398   \n2   3        1.658054        1.133251        1.001653        1.077331   \n3   4       -0.212713        0.246720        0.700630       -0.088591   \n4   5        0.002237       -0.825004        1.131528        0.224879   \n\n   BlendProperty5  BlendProperty6  BlendProperty7  BlendProperty8  \\\n0        0.400821        0.533569        0.539430        0.271776   \n1       -0.694031       -0.139598       -1.181084       -1.192864   \n2        1.465595        1.531201        1.020380        1.800945   \n3        1.756365       -0.286535        0.711253        1.099794   \n4        2.329752       -0.006683        1.125447        0.143765   \n\n   BlendProperty9  BlendProperty10  \n0       -0.315240         0.307335  \n1       -0.609116         0.023549  \n2        0.341868         2.186611  \n3        0.763524        -0.931361  \n4       -0.658237         0.963259  \n","output_type":"stream"}],"execution_count":16}]}