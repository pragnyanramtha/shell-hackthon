{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12405754,"sourceType":"datasetVersion","datasetId":7823452},{"sourceId":12406847,"sourceType":"datasetVersion","datasetId":7824237}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport lightgbm as lgb # The new weapon\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.multioutput import MultiOutputRegressor\n\ntrain_df = pd.read_csv('/kaggle/input/training/train.csv',skiprows=0)\n\nY = train_df.filter(like='Blend')\n\nX = train_df.drop(Y.columns , axis = 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T11:31:38.325994Z","iopub.execute_input":"2025-07-08T11:31:38.326745Z","iopub.status.idle":"2025-07-08T11:31:38.375138Z","shell.execute_reply.started":"2025-07-08T11:31:38.326719Z","shell.execute_reply":"2025-07-08T11:31:38.374459Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"def feature_expansion(X):\n\n    x_engg = X.copy()\n    new_features = []\n    \n    for i in range(1,6):\n        fractionc = f\"Component{i}_fraction\"\n        proportionc = X[fractionc]\n        propertyc  = f\"Component{i}_Property\"\n        ComponentPropertiesc = X.filter(like=propertyc)  \n        partial_componets = proportionc.values.reshape(-1, 1) * ComponentPropertiesc\n    \n        partial_componets.columns = [f'weighted_C{i}_P{j}' for j in range(1, 11)]\n        \n        new_features.append(partial_componets)\n    \n    X_final = pd.concat([x_engg] + new_features, axis=1 )\n    return X_final","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T11:31:38.376410Z","iopub.execute_input":"2025-07-08T11:31:38.376658Z","iopub.status.idle":"2025-07-08T11:31:38.382233Z","shell.execute_reply.started":"2025-07-08T11:31:38.376635Z","shell.execute_reply":"2025-07-08T11:31:38.381374Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"SHIFT_VALUE = 10\nY_shifted = Y + SHIFT_VALUE\nX_final = feature_expansion(X)\nX_train, X_val, y_train_shifted, y_val_shifted = train_test_split(\n    X_final, Y_shifted, test_size=0.2, random_state=42\n)\n\n\ncore_lgbm = lgb.LGBMRegressor(\n    objective='mae',\n    n_estimators=1000,\n    learning_rate=0.05,\n    num_leaves=31,\n    n_jobs=-1,\n    seed=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T11:31:38.383019Z","iopub.execute_input":"2025-07-08T11:31:38.383291Z","iopub.status.idle":"2025-07-08T11:31:38.415013Z","shell.execute_reply.started":"2025-07-08T11:31:38.383273Z","shell.execute_reply":"2025-07-08T11:31:38.413982Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"multi_output_lgbm = MultiOutputRegressor(core_lgbm)\n\nmulti_output_lgbm.fit(X_train, y_train_shifted)\n\nval_predictions_shifted = multi_output_lgbm.predict(X_val) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T11:31:38.416660Z","iopub.execute_input":"2025-07-08T11:31:38.416981Z","iopub.status.idle":"2025-07-08T11:32:39.327451Z","shell.execute_reply.started":"2025-07-08T11:31:38.416953Z","shell.execute_reply":"2025-07-08T11:32:39.326772Z"}},"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002082 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 9.985353\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002008 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 9.985323\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001969 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 10.119850\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001908 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 9.959146\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001960 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 9.739591\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001932 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 9.943575\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001961 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 10.107700\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001974 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 9.999769\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001966 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 9.976557\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001985 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 25727\n[LightGBM] [Info] Number of data points in the train set: 1600, number of used features: 105\n[LightGBM] [Info] Start training from score 9.989404\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"val_predictions_original = val_predictions_shifted - SHIFT_VALUE\ny_val_original = y_val_shifted - SHIFT_VALUE\n\nmape_cost = mean_absolute_percentage_error(y_val_original, val_predictions_original)\nprint(f\"Final Validation MAPE: {mape_cost:.4f}\")\n\nreference_cost = 2.72\nleaderboard_score = 100 - (90 * mape_cost) / reference_cost\nfinal_score = max(10, leaderboard_score)\n\nprint(f\"\\nScore with LightGBM: {final_score:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-08T11:32:39.328164Z","iopub.execute_input":"2025-07-08T11:32:39.328393Z","iopub.status.idle":"2025-07-08T11:32:39.337377Z","shell.execute_reply.started":"2025-07-08T11:32:39.328375Z","shell.execute_reply":"2025-07-08T11:32:39.336530Z"}},"outputs":[{"name":"stdout","text":"Final Validation MAPE: 2.3061\n\nScore with LightGBM: 23.70\n","output_type":"stream"}],"execution_count":33}]}