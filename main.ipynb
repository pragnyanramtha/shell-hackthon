{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4bf7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Component1_fraction  Component2_fraction  Component3_fraction  \\\n",
      "0                    0.21                 0.00                 0.42   \n",
      "1                    0.02                 0.33                 0.19   \n",
      "2                    0.08                 0.08                 0.18   \n",
      "3                    0.25                 0.42                 0.00   \n",
      "4                    0.26                 0.16                 0.08   \n",
      "...                   ...                  ...                  ...   \n",
      "1995                 0.50                 0.12                 0.00   \n",
      "1996                 0.19                 0.31                 0.00   \n",
      "1997                 0.38                 0.06                 0.14   \n",
      "1998                 0.50                 0.16                 0.00   \n",
      "1999                 0.00                 0.34                 0.21   \n",
      "\n",
      "      Component4_fraction  Component5_fraction  Component1_Property1  \\\n",
      "0                    0.25                 0.12             -0.021782   \n",
      "1                    0.46                 0.00             -0.224339   \n",
      "2                    0.50                 0.16              0.457763   \n",
      "3                    0.07                 0.26             -0.577734   \n",
      "4                    0.50                 0.00              0.120415   \n",
      "...                   ...                  ...                   ...   \n",
      "1995                 0.26                 0.12              0.279523   \n",
      "1996                 0.37                 0.13             -0.887185   \n",
      "1997                 0.31                 0.11              0.568978   \n",
      "1998                 0.18                 0.16             -0.067453   \n",
      "1999                 0.45                 0.00              0.284090   \n",
      "\n",
      "      Component2_Property1  Component3_Property1  Component4_Property1  \\\n",
      "0                 1.981251              0.020036              0.140315   \n",
      "1                 1.148036             -1.107840              0.149533   \n",
      "2                 0.242591             -0.922492              0.908213   \n",
      "3                -0.930826              0.815284              0.447514   \n",
      "4                 0.666268             -0.626934              2.725357   \n",
      "...                    ...                   ...                   ...   \n",
      "1995             -0.054170             -0.391227              0.400222   \n",
      "1996              0.610050              0.178606              1.083154   \n",
      "1997             -0.196759             -0.646318             -0.980070   \n",
      "1998              0.321977             -0.137535              0.238507   \n",
      "1999              0.189099             -0.831267             -1.084474   \n",
      "\n",
      "      Component5_Property1  ...  BlendProperty1  BlendProperty2  \\\n",
      "0                 1.032029  ...        0.489143        0.607589   \n",
      "1                -0.354000  ...       -1.257481       -1.475283   \n",
      "2                 0.972003  ...        1.784349        0.450467   \n",
      "3                 0.455717  ...       -0.066422        0.483730   \n",
      "4                 0.392259  ...       -0.118913       -1.172398   \n",
      "...                    ...  ...             ...             ...   \n",
      "1995              1.032029  ...       -0.028366       -0.327297   \n",
      "1996             -2.822749  ...       -0.449245        0.156778   \n",
      "1997              1.032029  ...        0.029135        0.164890   \n",
      "1998              0.017455  ...       -0.232960       -0.464947   \n",
      "1999              0.845087  ...       -1.797180       -1.312212   \n",
      "\n",
      "      BlendProperty3  BlendProperty4  BlendProperty5  BlendProperty6  \\\n",
      "0           0.321670       -1.236055        1.601132        1.384662   \n",
      "1          -0.437385       -1.402911        0.147941       -1.143244   \n",
      "2           0.622687        1.375614       -0.428790        1.161616   \n",
      "3          -1.865442       -0.046295       -0.163820       -0.209693   \n",
      "4           0.301785       -1.787407       -0.493361       -0.528049   \n",
      "...              ...             ...             ...             ...   \n",
      "1995       -0.316933       -1.294092       -0.530259       -0.421526   \n",
      "1996       -0.367445       -0.938615       -0.577451       -0.209996   \n",
      "1997       -0.092942       -1.134490       -0.437479       -0.695636   \n",
      "1998        0.112536       -0.793522       -0.811272       -1.194914   \n",
      "1999       -0.511896       -1.450066       -0.365154       -1.087937   \n",
      "\n",
      "      BlendProperty7  BlendProperty8  BlendProperty9  BlendProperty10  \n",
      "0           0.305850        0.193460        0.580374        -0.762738  \n",
      "1          -0.439171       -1.379041       -1.280989        -0.503625  \n",
      "2           0.601289        0.872950        0.660000         2.024576  \n",
      "3          -1.840566        0.300293       -0.351336        -1.551914  \n",
      "4           0.286344       -0.265192        0.430513         0.735073  \n",
      "...              ...             ...             ...              ...  \n",
      "1995       -0.320869        0.709627       -0.737244        -0.744289  \n",
      "1996       -0.370505       -0.195531       -0.032834         0.269718  \n",
      "1997       -0.101073        0.063650        0.624368        -0.477053  \n",
      "1998        0.100644        0.760116       -0.751394        -0.857598  \n",
      "1999       -0.512119       -0.582473       -0.834879        -0.272462  \n",
      "\n",
      "[2000 rows x 65 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from scipy.stats import pearsonr\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# Attempt to import cupy for GPU acceleration of data arrays\n",
    "try:\n",
    "    import cupy as cp\n",
    "    CUPY_AVAILABLE = True\n",
    "    print(\"CuPy found. GPU data acceleration is enabled.\")\n",
    "except ImportError:\n",
    "    CUPY_AVAILABLE = False\n",
    "    print(\"CuPy not found. Predictions will use CPU data, which may be slower.\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device for GPU acceleration for PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using PyTorch device: {device}\")\n",
    "\n",
    "# =========================================================================\n",
    "# Helper Classes\n",
    "# =========================================================================\n",
    "\n",
    "class FuelBlendingDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.FloatTensor(X.values if isinstance(X, pd.DataFrame) else X)\n",
    "        self.y = torch.FloatTensor(y.values if isinstance(y, pd.DataFrame) else y) if y is not None else None\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.y[idx]) if self.y is not None else self.X[idx]\n",
    "\n",
    "class MultiTargetModelWrapper:\n",
    "    \"\"\"A generic wrapper for multi-target models trained individually.\"\"\"\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [model.predict(X) for model in self.models]\n",
    "        if isinstance(predictions[0], np.ndarray):\n",
    "            return np.column_stack(predictions)\n",
    "        elif CUPY_AVAILABLE and isinstance(predictions[0], cp.ndarray):\n",
    "            return cp.column_stack(predictions)\n",
    "        return np.column_stack(predictions)\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, attention_dim=64):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.attention = nn.Sequential(nn.Linear(input_dim, attention_dim), nn.Tanh(), nn.Linear(attention_dim, 1, bias=False))\n",
    "    def forward(self, x):\n",
    "        weights = torch.softmax(self.attention(x), dim=1)\n",
    "        return x * weights\n",
    "\n",
    "class AdvancedNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims=[512, 256, 128, 64], dropout_rate=0.3):\n",
    "        super(AdvancedNeuralNetwork, self).__init__()\n",
    "        layers = [nn.Linear(input_dim, hidden_dims[0]), nn.BatchNorm1d(hidden_dims[0]), nn.ReLU(), nn.Dropout(dropout_rate)]\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            layers.extend([nn.Linear(hidden_dims[i], hidden_dims[i+1]), nn.BatchNorm1d(hidden_dims[i+1]), nn.ReLU(), nn.Dropout(dropout_rate)])\n",
    "        self.main_layers = nn.Sequential(*layers)\n",
    "        self.attention = AttentionLayer(hidden_dims[-1])\n",
    "        self.output_layers = nn.Sequential(nn.Linear(hidden_dims[-1], hidden_dims[-1]//2), nn.ReLU(), nn.Dropout(dropout_rate/2), nn.Linear(hidden_dims[-1]//2, output_dim))\n",
    "        self.residual = nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        main_out = self.attention(self.main_layers(x))\n",
    "        main_pred = self.output_layers(main_out)\n",
    "        resid_pred = self.residual(x)\n",
    "        alpha = torch.sigmoid(main_pred.mean(dim=1, keepdim=True))\n",
    "        return alpha * main_pred + (1 - alpha) * resid_pred\n",
    "\n",
    "# =========================================================================\n",
    "# Main Predictor Class\n",
    "# =========================================================================\n",
    "\n",
    "class FuelBlendingPredictor:\n",
    "    def __init__(self, use_gpu=True):\n",
    "        self.device = device if use_gpu and torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.models = {}\n",
    "        self.scalers = {}\n",
    "        self.neural_net = None\n",
    "        self.best_features = None\n",
    "        self.target_names = None\n",
    "        \n",
    "    def create_advanced_features(self, df):\n",
    "        df_features = df.copy()\n",
    "        blend_cols = [col for col in df.columns if 'fraction' in col]\n",
    "        if not blend_cols: return df_features # Return if no fraction columns found\n",
    "        df_features['blend_entropy'] = -np.sum(df[blend_cols] * np.log(df[blend_cols] + 1e-8), axis=1)\n",
    "        df_features['blend_gini'] = 1 - np.sum(df[blend_cols]**2, axis=1)\n",
    "        return df_features\n",
    "    \n",
    "    def select_best_features(self, X, y, max_features=150):\n",
    "        X = X.fillna(0) # Simple imputation for feature selection\n",
    "        feature_scores = pd.Series(index=X.columns, dtype=float).fillna(0)\n",
    "        mi_scores = mutual_info_regression(X, y.mean(axis=1), random_state=42)\n",
    "        feature_scores += pd.Series(mi_scores, index=X.columns)\n",
    "        f_selector = SelectKBest(score_func=f_regression, k='all')\n",
    "        f_selector.fit(X, y.mean(axis=1))\n",
    "        feature_scores += pd.Series(f_selector.scores_ / 1000, index=X.columns).fillna(0)\n",
    "        selected_features = feature_scores.nlargest(max_features).index.tolist()\n",
    "        print(f\"Selected {len(selected_features)} features out of {len(X.columns)}\")\n",
    "        return selected_features\n",
    "    \n",
    "    def train_neural_network(self, X_train, y_train, X_val, y_val, epochs=250, batch_size=64, lr=0.001):\n",
    "        print(\"Training Neural Network...\")\n",
    "        train_loader = DataLoader(FuelBlendingDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(FuelBlendingDataset(X_val, y_val), batch_size=batch_size, shuffle=False)\n",
    "        self.neural_net = AdvancedNeuralNetwork(X_train.shape[1], y_train.shape[1]).to(self.device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.AdamW(self.neural_net.parameters(), lr=lr, weight_decay=1e-4)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=15, factor=0.5)\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter, patience = 0, 30\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.neural_net.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(self.device), y_batch.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(self.neural_net(X_batch), y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            self.neural_net.eval()\n",
    "            val_loss = sum(criterion(self.neural_net(X.to(self.device)), y.to(self.device)).item() for X, y in val_loader) / len(val_loader)\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss, patience_counter = val_loss, 0\n",
    "                torch.save(self.neural_net.state_dict(), 'best_neural_net.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        self.neural_net.load_state_dict(torch.load('best_neural_net.pth'))\n",
    "        print(f\"Neural Network training completed. Best validation loss: {best_val_loss:.6f}\")\n",
    "    \n",
    "    def train_gradient_boosting_models(self, X_train, y_train, X_val, y_val):\n",
    "        print(\"Training Gradient Boosting Models...\")\n",
    "        X_val_device = cp.asarray(X_val) if (self.device.type == 'cuda' and CUPY_AVAILABLE) else X_val\n",
    "\n",
    "        # --- Per-target training for XGBoost ---\n",
    "        print(\"Training xgb with per-target early stopping...\")\n",
    "        start_time = time.time()\n",
    "        xgb_params = {'n_estimators': 2000, 'learning_rate': 0.05, 'max_depth': 7, 'subsample': 0.8,\n",
    "                      'colsample_bytree': 0.8, 'random_state': 42, 'tree_method': 'hist',\n",
    "                      'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "                      'early_stopping_rounds': 50} # Correct way to set early stopping\n",
    "        xgb_models = []\n",
    "        for i in range(y_train.shape[1]):\n",
    "            model = xgb.XGBRegressor(**xgb_params)\n",
    "            model.fit(X_train, y_train.iloc[:, i],\n",
    "                      eval_set=[(X_val, y_val.iloc[:, i])], verbose=False)\n",
    "            xgb_models.append(model)\n",
    "        self.models['xgb'] = MultiTargetModelWrapper(xgb_models)\n",
    "        print(f\"xgb training completed in {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "        # --- Per-target training for CatBoost ---\n",
    "        print(\"Training catboost with per-target early stopping...\")\n",
    "        start_time = time.time()\n",
    "        cb_params = {'iterations': 2000, 'learning_rate': 0.05, 'depth': 7, 'l2_leaf_reg': 3,\n",
    "                     'random_seed': 42, 'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
    "                     'verbose': 0, 'allow_writing_files': False, 'early_stopping_rounds': 50}\n",
    "        cb_models = []\n",
    "        for i in range(y_train.shape[1]):\n",
    "            model = cb.CatBoostRegressor(**cb_params)\n",
    "            model.fit(X_train, y_train.iloc[:, i],\n",
    "                      eval_set=[(X_val, y_val.iloc[:, i])], verbose=False)\n",
    "            cb_models.append(model)\n",
    "        self.models['catboost'] = MultiTargetModelWrapper(cb_models)\n",
    "        print(f\"catboost training completed in {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "        # --- Per-target training for LightGBM ---\n",
    "        print(\"Training lgb with per-target early stopping...\")\n",
    "        start_time = time.time()\n",
    "        lgb_params = {'n_estimators': 2500, 'learning_rate': 0.05, 'num_leaves': 40, 'max_depth': 8,\n",
    "                      'max_bin': 128, 'subsample': 0.8, 'colsample_bytree': 0.8, 'random_state': 42,\n",
    "                      'device': 'gpu' if torch.cuda.is_available() else 'cpu', 'verbose': -1}\n",
    "        lgb_models = []\n",
    "        for i in range(y_train.shape[1]):\n",
    "            model = lgb.LGBMRegressor(**lgb_params)\n",
    "            model.fit(X_train, y_train.iloc[:, i],\n",
    "                      eval_set=[(X_val, y_val.iloc[:, i])],\n",
    "                      callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "            lgb_models.append(model)\n",
    "        self.models['lgb'] = MultiTargetModelWrapper(lgb_models)\n",
    "        print(f\"lgb training completed in {time.time() - start_time:.2f}s.\")\n",
    "\n",
    "        # --- Training for RandomForest ---\n",
    "        print(\"Training rf...\")\n",
    "        start_time = time.time()\n",
    "        rf = RandomForestRegressor(n_estimators=150, max_depth=12, min_samples_leaf=3, random_state=42, n_jobs=-1)\n",
    "        self.models['rf'] = MultiOutputRegressor(rf, n_jobs=-1).fit(X_train, y_train)\n",
    "        print(f\"rf training completed in {time.time() - start_time:.2f}s.\")\n",
    "    \n",
    "    def create_ensemble_predictions(self, X):\n",
    "        predictions, weights = [], []\n",
    "        use_cupy = self.device.type == 'cuda' and CUPY_AVAILABLE\n",
    "        X_device = cp.asarray(X) if use_cupy else X\n",
    "\n",
    "        if self.neural_net:\n",
    "            self.neural_net.eval()\n",
    "            with torch.no_grad():\n",
    "                nn_pred = self.neural_net(torch.FloatTensor(X.values).to(self.device)).cpu().numpy()\n",
    "                predictions.append(nn_pred)\n",
    "                weights.append(0.35)\n",
    "        \n",
    "        for name, model in self.models.items():\n",
    "            is_gpu_model = name in ['xgb', 'lgb', 'catboost']\n",
    "            pred_device = model.predict(X_device if (use_cupy and is_gpu_model) else X)\n",
    "            pred = pred_device.get() if (use_cupy and isinstance(pred_device, cp.ndarray)) else pred_device\n",
    "            predictions.append(pred)\n",
    "            weights.append(0.20 if is_gpu_model else 0.05)\n",
    "        \n",
    "        return pd.DataFrame(np.average(predictions, axis=0, weights=np.array(weights)/np.sum(weights)), columns=self.target_names)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.target_names = y.columns.tolist()\n",
    "        X_processed = self.create_advanced_features(X)\n",
    "        self.best_features = self.select_best_features(X_processed, y)\n",
    "        X_selected = X_processed[self.best_features]\n",
    "        \n",
    "        self.scalers['feature_scaler'] = RobustScaler()\n",
    "        X_scaled = pd.DataFrame(self.scalers['feature_scaler'].fit_transform(X_selected), columns=self.best_features)\n",
    "        self.scalers['target_scaler'] = RobustScaler()\n",
    "        y_scaled = pd.DataFrame(self.scalers['target_scaler'].fit_transform(y), columns=y.columns)\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split(X_scaled, y_scaled, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.train_neural_network(X_train, y_train, X_val, y_val)\n",
    "        self.train_gradient_boosting_models(X_train, y_train, X_val, y_val)\n",
    "        \n",
    "        val_preds_scaled = self.create_ensemble_predictions(X_val)\n",
    "        val_preds_orig = self.scalers['target_scaler'].inverse_transform(val_preds_scaled)\n",
    "        y_val_orig = self.scalers['target_scaler'].inverse_transform(y_val)\n",
    "        final_mape = mean_absolute_percentage_error(y_val_orig, val_preds_orig)\n",
    "        print(f\"\\nðŸŽ¯ Final Ensemble MAPE on Validation Set: {final_mape:.6f}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_processed = self.create_advanced_features(X)\n",
    "        X_selected = X_processed[self.best_features]\n",
    "        X_scaled = pd.DataFrame(self.scalers['feature_scaler'].transform(X_selected), columns=self.best_features)\n",
    "        preds_scaled = self.create_ensemble_predictions(X_scaled)\n",
    "        return pd.DataFrame(self.scalers['target_scaler'].inverse_transform(preds_scaled), columns=self.target_names)\n",
    "\n",
    "# =========================================================================\n",
    "# Main Execution Block\n",
    "# =========================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸš€ Starting Fuel Blending ML Pipeline\")\n",
    "    try:\n",
    "        # Updated data loading paths\n",
    "        train_df = pd.read_csv('/kaggle/input/training/train.csv')\n",
    "        test_df = pd.read_csv('/kaggle/input/testing/test.csv')\n",
    "        sample_submission = pd.read_csv('/kaggle/input/samplesubmission/sample_solution.csv')\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        print(\"Please ensure the Kaggle dataset is correctly mounted at /kaggle/input/\")\n",
    "        return\n",
    "        \n",
    "    target_columns = [col for col in train_df.columns if 'BlendProperty' in col]\n",
    "    if not target_columns:\n",
    "        # Fallback for different naming conventions\n",
    "        target_columns = sample_submission.columns.drop('ID').tolist()\n",
    "\n",
    "    feature_columns = [col for col in train_df.columns if col not in target_columns and 'ID' not in col]\n",
    "    \n",
    "    # Align test set columns with training set columns\n",
    "    test_features = test_df[feature_columns]\n",
    "\n",
    "    model = FuelBlendingPredictor(use_gpu=True)\n",
    "    model.fit(train_df[feature_columns], train_df[target_columns])\n",
    "    \n",
    "    predictions = model.predict(test_features)\n",
    "    \n",
    "    submission_id_col = 'ID' if 'ID' in test_df.columns else test_df.index.name\n",
    "    if submission_id_col not in test_df:\n",
    "        submission = pd.DataFrame({'ID': test_df.index})\n",
    "    else:\n",
    "        submission = pd.DataFrame({'ID': test_df[submission_id_col]})\n",
    "        \n",
    "    submission = pd.concat([submission, predictions], axis=1)\n",
    "        \n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"\\nðŸ’¾ Submission file 'submission.csv' saved successfully.\")\n",
    "    print(submission.head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
